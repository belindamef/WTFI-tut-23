---
fontsize: 8pt
bibliography: ../Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: ../Header.tex
classoption: t  
---


```{r, include = F}
source("../R_common.R")
abb_dir = file.path(dirname(getwd()), "Abbildungen")
```

#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics(file.path(abb_dir,"otto.png"))
```

\vspace{2mm}

\huge
Tutorium 

\Large
Wahrscheinlichketstheorie und Frequentistische Inferenz
\vspace{4mm}

\normalsize
BSc Psychologie WiSe 2022/23

\vspace{12mm}
\normalsize
Belinda Fleischmann

\vspace{3mm}
\scriptsize
Inhalte basieren auf Kursmaterialien für [WTFI](https://www.ipsy.ovgu.de/Institut/Abteilungen+des+Institutes/Methodenlehre+I+_+Experimentelle+und+Neurowissenschaftliche+Psychologie/Lehre/Wintersemester+2023/Wahrscheinlichkeitstheorie+und+Frequentistische+Inferenz.html) von [Dirk Ostwald](https://www.ipsy.ovgu.de/Institut/Abteilungen+des+Institutes/Methodenlehre+I+_+Experimentelle+und+Neurowissenschaftliche+Psychologie/Team/Dirk+Ostwald.html), lizenziert unter [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.de)



#  {.plain}
\vfill
\center
\huge
\text{(5) Multivariate Verteilungen}



# Selbstkontrollfragen
\setstretch{2}

\small
1. \justifying Definieren Sie den Begriff des Zufallsvektors.
2. Definieren Sie den Begriff der multivariaten Verteilung eines Zufallsvektors.
3. Definieren Sie den Begriff der multivariaten WMF.
4. Definieren Sie den Begriff der multivariaten WDF.
5. Definieren Sie den Begriff der univariaten Marginalverteilung eines Zufallsvektors.
6. Wie berechnet man die WMF der $i$ten Komponente eines diskreten Zufallsvektors?
7. Wie berechnet man die WDF der $i$ten Komponente eines kontinuierlichen Zufallsvektors?
8. Definieren Sie den Begriff der Unabhängigkeit zweier Zufallsvariablen.
9. Wie erkennt man an der gemeinsamen WMF oder WDF eines zweidimensionalen Zufallsvektors, ob die
Komponenten des Zufallsvektors unabhängig sind oder nicht?
10. Definieren Sie den Begriff der Unabhängigkeit von $n$ Zufallsvariablen.
11. Definieren Sie den Begriff $n$ unabhängig und identisch verteilter Zufallsvariablen.


# \textcolor{darkcyan}{Wdhl.: Modell und Realität}
```{r, echo = FALSE, out.width = "95%"}
knitr::include_graphics(file.path(abb_dir, "wtfi_5_wahrscheinlichkeitstheorie_modell.pdf"))
```



# \textcolor{darkcyan}{Wdhl.: Modell und Realität} 
\vspace{4mm}
```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics(file.path(abb_dir, "wtfi_5_wahrscheinlichkeitstheorie_modell_beispiel.pdf"))
```



# \textcolor{darkblue}{SKF 1}. *Zufallsvektor*

\large
\color{darkblue} 1. \justifying Definieren Sie den Begriff des Zufallsvektors.


\vspace{3mm}
\color{black}
\footnotesize

\begin{definition}[Zufallsvektor]
\justifying
$(\Omega, \mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum und 
$(\mathcal{X},\mathcal{S})$ sei ein $n$-dimensionaler Messraum. 
Ein $n$-dimensionaler \textit{Zufallsvektor} ist definiert als eine Abbildung
\begin{equation}
\xi:\Omega \to \mathcal{X}, \omega \mapsto \xi(\omega) :=
\begin{pmatrix}
\xi_1(\omega) \\
\vdots  	\\
\xi_n(\omega)
\end{pmatrix}
\end{equation}
mit der \textit{Messbarkeitseigenschaft}
\begin{equation}
\{\omega \in \Omega|\xi(\omega) \in S \} \in \mathcal{A} \mbox{ für alle } S \in \mathcal{S}.
\end{equation}
\end{definition}

\color{darkcyan}



# \textcolor{darkcyan}{Veranschaulichung: Zufallsvektor} 

```{r, echo = FALSE,out.width = "90%"}
knitr::include_graphics(file.path(abb_dir, "wtfi_5_zufallsvektor.pdf"))
```



# \textcolor{darkcyan}{Wdhl.: Zufallsvariable} 

\vspace{3mm}
\footnotesize

\begin{definition}[Zufallsvariable]
\justifying
Es sei $(\Omega, \mathcal{A}, \mathbb{P})$ ein Wahrscheinlichkeitsraum und
$(\mathcal{X},\mathcal{S})$ ein \textit{Messraum}. Dann ist eine \textit{Zufallsvariable (ZV)}
definiert als eine Abbildung $\xi:\Omega \to \mathcal{X}$ mit der \textit{Messbarkeitseigenschaft}
\begin{equation}
\{\omega \in \Omega|\xi(\omega) \in S \} \in \mathcal{A} \mbox{ für alle } S \in \mathcal{S}.
\end{equation}
\end{definition}


```{r, echo = FALSE, out.width = "60%" }
knitr::include_graphics(file.path(abb_dir, "wtfi_4_zufallsvariable.pdf"))
```



# \textcolor{darkblue}{SKF 2}. *Multivariate Verteilung eines Zufallsvektors*

\large
\color{darkblue} 2. Definieren Sie den Begriff der multivariaten Verteilung eines Zufallsvektors.


\vspace{3mm}
\color{black}
\footnotesize

\begin{definition}[Multivariate Verteilung]
\justifying
$(\Omega, \mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum, 
$(\mathcal{X},\mathcal{S})$ sei ein $n$-dimensionaler Messraum und
\begin{equation}
\xi : \Omega \to \mathcal{X}, \omega \mapsto \xi(\omega)
\end{equation}
sei ein Zufallsvektor. Dann heißt das Wahrscheinlichkeitsmaß $\mathbb{P}_\xi$, 
definiert durch
\begin{equation}
\mathbb{P}_\xi : \mathcal{S} \to [0,1], S \mapsto \mathbb{P}_\xi(S)
:= \mathbb{P}(\xi^{-1}(S))
 = \mathbb{P}\left(\{\omega \in \Omega|\xi(\omega) \in S\}\right)
\end{equation}
die \textit{multivariate Verteilung des Zufallsvektor $\xi$}.
\end{definition}



# \textcolor{darkblue}{SKF 3}. *Multivariate WMF*

\large
\color{darkblue} 3. Definieren Sie den Begriff der multivariaten WMF.


\vspace{3mm}
\color{black}
\footnotesize

\begin{definition}[Diskreter Zufallsvektor, Multivariate WMF]
\justifying
$\xi$ sei ein Zufallsvektor mit Ergebnisraum $\mathcal{X}$. $\xi$ heißt 
\textit{diskreter Zufallsvektor} wenn der Ergebnisraum $\mathcal{X}$ endlich oder
abzählbar ist und eine Funktion 
\begin{equation}
p_\xi : \mathcal{X} \to [0,1], x \mapsto p_\xi(x)  
\end{equation}
existiert, für die gilt
\begin{itemize}
\item[(1)] $\sum_{x \in \mathcal{X}}p(x) = 1$ und
\item[(2)] $\mathbb{P}_\xi(\xi = x) = p(x)$ für alle $x \in \mathcal{X}$.
\end{itemize}
Ein entsprechende Funktion $p$ heißt \textit{multivariate Wahrscheinlichkeitsmassefunktion (WMF)} von $\xi$.
\end{definition}



# \textcolor{darkblue}{SKF 4}. *Multivariate WDF*

\large
\color{darkblue} 4. Definieren Sie den Begriff der multivariaten WDF.


\vspace{3mm}
\color{black}
\footnotesize

\begin{definition}[Kontinuierlicher Zufallsvektor, Multivariate WDF]
\justifying
Ein Zufallsvektor $\xi$ heißt \textit{kontinuierlich}, wenn $\mathbb{R}^n$ der 
Ergebnisraum von $\xi$ ist und eine Funktion  
\begin{equation}
p_\xi : \mathbb{R}^n \to \mathbb{R}_{\ge 0}, x \mapsto p_\xi(x),
\end{equation}
existiert, für die gilt
\begin{itemize}
\item[(1)] $\int_{\mathbb{R}^n} p_\xi(x)\,dx = 1$ und
\item[(2)] $\mathbb{P}_\xi(x_1 \le \xi \le x_2)
		    = \int_{x_{1_1}}^{x_{2_1}} \cdots \int_{x_{1_n}}^{x_{2_n}} p_\xi(s_1,...,s_n)\,ds_1 \cdots ds_n$.
\end{itemize}
Eine entsprechende Funktion $p$ heißt \textit{multivariate Wahrscheinlichkeitsdichtefunktion (WDF)} von $\xi$.
\end{definition}



# \textcolor{darkblue}{SKF 5}. *Univariate Marginalverteilung eines Zufallsvektors*

\large
\color{darkblue} 5. Definieren Sie den Begriff der univariaten Marginalverteilung eines Zufallsvektors.


\vspace{3mm}
\color{black}
\footnotesize

\begin{definition}[Univariate Marginalverteilung]
\justifying
$(\Omega, \mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum, 
$(\mathcal{X}, \mathcal{S})$ sei ein $n$-dimensionaler Messraum, 
$\xi:\Omega \to \mathcal{X}$ sei ein Zufallsvektor, $\mathbb{P}_\xi$ sei die 
Verteilung von $\xi$, $\mathcal{X}_i \subset \mathcal{X}$ sei der Ergebnisraum der 
$i$ten Komponente $\xi_i$ von $\xi$, und $\mathcal{S}_i$ sei eine 
$\sigma$-Algebra auf $\xi_i$. Dann heißt die durch
\begin{equation}
\mathbb{P}_{\xi_i} : \mathcal{S}_i \to [0,1],
S \mapsto  \mathbb{P}_\xi\left(\mathcal{X}_1
                     \times
                     \cdots
                     \times
                     \mathcal{X}_{i-1}
                     \times S
                     \times \mathcal{X}_{i+1}
                     \times \cdots
                     \times \mathcal{X}_n\right)
\mbox{ für } S \in \mathcal{S}_i
\end{equation}
definierte Verteilung die \textit{$i$te univariate Marginalverteilung von $\xi$}.
\end{definition}



# \color{darkcyan}{Beispiel}
Beispiel (Marginale Wahrscheinlichkeitsmassefunktionen)

\vspace{2mm}

\small
\justifying

Wir betrachten erneut den zweidimensionalen Zufallsvektor $\xi:= (\xi_1,\xi_2)$ der 
Werte in $\mathcal{X} := \mathcal{X}_1 \times \mathcal{X}_2$ annimmt, wobei 
$\mathcal{X}_1 := \{1,2,3\}$ und $\mathcal{X}_2 = \{1,2,3,4\}$ seien. 
\vspace{1mm}

Basierend auf der oben definierten WMF ergeben sich folgende marginale WMFen 
$p_{\xi_1}$ und $p_{\xi_2}$:
\vspace{2mm}

\begin{table}\label{tab:wmf_marginal}
\begin{center}
\begin{tabular}{|c|cccc|c|}
\hline
$p_\xi(x_1,x_2)$    & 	$x_2 = 1$ 	& 	$x_2 = 2$ 	& 	$x_2 = 3$ 	& 	$x_2 = 4$ 	& $p_{\xi_1}(x_1)$	\\\hline
$x_1 = 1$		    &	$0.1$		&	$0.0$		&	$0.2$		&	$0.1$		& $0.4$				\\
$x_1 = 2$		    &	$0.1$		&	$0.2$		&	$0.0$		&	$0.0$		& $0.3$				\\
$x_1 = 3$		    &	$0.0$		&	$0.1$		&	$0.1$		&	$0.1$		& $0.3$				\\\hline
$p_{\xi_2}(x_2)$	&   $0.2$	 	&   $0.3$ 		&   $0.3$ 		&   $0.2$       & 					\\\hline
\end{tabular}
\end{center}
\end{table}
\vspace{2mm}

Man beachte, dass $\sum_{x_1 = 1}^3 p_{\xi_1}(x_1) = 1$ und $\sum_{x_2 = 1}^3 p_{\xi_2}(x_2) = 1$ gilt.



# \textcolor{darkblue}{SKF 6}. *WMF der $i$ten Komponente* | *Diskreter Zufallsvektor*

\large
\color{darkblue} 6. Wie berechnet man die WMF der $i$ten Komponente eines diskreten Zufallsvektors?


\vspace{3mm}
\color{black}
\footnotesize

$\xi= (\xi_1,...,\xi_n)$ sei ein $n$-dimensionaler diskreter Zufallsvektor
mit Wahrscheinlichkeitsmassefunktion $p_\xi$ und Komponentenergebnisräumen 
$\mathcal{X}_1, ..., \mathcal{X}_n$. Dann ergibt sich die 
Wahrscheinlichkeitsmassefunktion der $i$ten Komponente $\xi_i$ von $\xi$ als
\begin{multline}
p_{\xi_i} : \mathcal{X}_i \to [0,1], x_i \mapsto p_{\xi_i}(x_i) := \\
\sum_{x_1} \cdots \sum_{x_{i-1}} \sum_{x_{i+1}} \cdots \sum_{x_n} p_\xi(x_1,...,x_{i-1},x_i,x_{i+1}, ...,x_n).
\end{multline}



# \textcolor{darkblue}{SKF 7}. *WDF der $i$ten Komponente* | *Kontinueierlicher Zufallsvektor*

\large
\color{darkblue} 7. Wie berechnet man die WDF der $i$ten Komponente eines kontinuierlichen Zufallsvektors?


\vspace{3mm}
\color{black}
\footnotesize

$\xi= (\xi_1,...,\xi_n)$ sei ein $n$-dimensionaler kontinuierlicher Zufallsvektor 
mit Wahrscheinlichkeitsdichtefunktion $p_\xi$ und Komponentenergebnisraum $\mathbb{R}$. 
Dann ergibt sich die Wahrscheinlichkeitsdichtefunktion der $i$ten Komponente $\xi_i$ von $\xi$ als
\begin{multline}
p_{\xi_i} : \mathbb{R} \to \mathbb{R}_{\ge 0},  x_i \mapsto p_{\xi_i}(x_i) :=  \\
\smallint_{x_1} \cdots \smallint_{x_{i-1}} \smallint_{x_{i+1}} \cdots \smallint_{x_n}
   p_\xi(x_1,..., x_{i-1},x_i,x_{i+1}, ...,x_n)
   \,dx_1...\,dx_{i-1}\,dx_{i+1}...\,dx_n.
\end{multline}



# \textcolor{darkblue}{SKF 8}. *Unabhängigkeit zweier ZVen*

\large
\color{darkblue} 8. Definieren Sie den Begriff der Unabhängigkeit zweier Zufallsvariablen.


\vspace{3mm}
\color{black}
\footnotesize

\begin{definition}[Unabhängige Zufallsvariablen]
\justifying
$(\Omega, \mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum und 
$\xi: = (\xi_1,\xi_2)$ ein zweidimensionaler Zufallsvektor. Die Zufallsvariablen 
$\xi_1,\xi_2$ mit Ergebnisräumen $\mathcal{X}_1, \mathcal{X}_2$ 
heißen \textit{unabhängig}, wenn für alle $S_1 \subseteq \mathcal{X}_1$ und 
$S_2 \subseteq \mathcal{X}_2$ gilt, dass
\begin{equation}
\mathbb{P}_\xi(\xi_1 \in S_1, \xi_2 \in S_2) =
\mathbb{P}_{\xi_1}(\xi_1 \in S_1)\mathbb{P}_{\xi_2}(\xi_2 \in S_2).
\end{equation}
\end{definition}


\color{darkcyan}
Bemerkung: Die Definition unabhängiger Zufallsvariablen ist analog zu der Definition unabhängiger Ereignisse.

Wdhl.: 

\begin{definition}[Unabhängige Ereignisse]
\justifying
Zwei Ereignisse $A \in \mathcal{A}$ and $B \in \mathcal{A}$ heißen
\textit{(stochastisch) unabhängig}, wenn
\begin{equation}
\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B).
\end{equation}
Eine Menge von Ereignissen $\{A_i|i \in I\}\subset \mathcal{A}$ mit beliebiger
Indexmenge $I$ heißt (stochastisch) unabhängig, wenn für jede endliche Untermenge
$J \subseteq I$ gilt, dass
\begin{equation}
\mathbb{P}\left(\cap_{j \in J} A_j \right) = \prod_{j \in J}\mathbb{P}(A_j).
\end{equation}
\end{definition}



# \textcolor{darkblue}{SKF 9}. *Unabhängigkeit der Komponenten eines Zufallsvektors*

\normalsize
\color{darkblue} 9. Wie erkennt man an der gemeinsamen WMF oder WDF eines zweidimensionalen Zufallsvektors, ob die Komponenten des Zufallsvektors unabhängig sind oder nicht?

\vspace{3mm}
\color{black}
\footnotesize

Unabhängigkeit zweier ZVen entspricht der Faktorisierung ihrer gemeinsam WMF/WDF. \textcolor{darkcyan}{\textit{Faktorisierung} bezeichnet die Produkteigenschaft $p_\xi(x_1,x_2) = p_{\xi_1}(x_1)p_{\xi_2}(x_2)$.} Mit anderen Worten, zwei ZVen oder die zwei Komponenten eines zweidimensionalen Zufallsvektors gelten als unabhängig, wenn die gemeinsame WMF oder WDF eines zweidimensionalen Zufallsvektors dem Produkt der marginalen WMF bzw. WDF entspricht. 

\begin{theorem}[Unabhängigkeit und Faktorisierung der WMF/WDF]
\justifying
\normalfont
(1) $\xi:= (\xi_1,\xi_2)$ sei ein diskreter Zufallsvektor mit Ergebnisraum
$\mathcal{X}_1 \times \mathcal{X}_2$, WMF $p_\xi$ und marginalen WMFen 
$p_{\xi_1}, p_{\xi_2}$. Dann gilt
\begin{multline}
\xi_1 \mbox{ und } \xi_2 \mbox{ sind unabhängige Zufallsvariablen} \Leftrightarrow \\
p_\xi(x_1,x_2) = p_{\xi_1}(x_1)p_{\xi_2}(x_2) \mbox{ für alle } (x_1,x_2) \in \mathcal{X}_1 \times \mathcal{X}_2.
\end{multline}
(2) $\xi:= (\xi_1,\xi_2)$ sei ein kontinuierlicher Zufallsvektor mit Ergebnisraum
$\mathbb{R}^2$, WDF $p_\xi$ und marginalen WDFen $p_{\xi_1}, p_{\xi_2}$. Dann gilt
\begin{multline}
\xi_1 \mbox{ und } \xi_2 \mbox{ sind unabhängige Zufallsvariablen } \Leftrightarrow \\
p_\xi(x_1,x_2) = p_{\xi_1}(x_1)p_{\xi_2}(x_2) \mbox{ für alle } (x_1,x_2) \in \mathbb{R}^2.
\end{multline}
\end{theorem}



# \textcolor{darkcyan}{Beispiel: Unabhängigkeit diskreter Zufallsvariablen}

\footnotesize

Wir betrachten den zweidimensionalen Zufallsvektor $\xi:= (\xi_1, \xi_2)$, 
der Werte in $\{1,2,3\} \times \{1,2,3,4\}$ annimmt, und dessen gemeinsame und
marginale WMFen die untenstehende Form haben
\vspace{1mm}

\begin{center}
\begin{tabular}{|c|cccc|c|}
\hline
$p_\xi(x_1,x_2)$	& 	$x_2 = 1$ 	& 	$x_2 = 2$ 	& 	$x_2 = 3$ 	& 	$x_2 = 4$ 	& $p_{\xi_1}(x_1)$	\\\hline
$x_1 = 1$		    &	$0.10$		&	$0.00$		&	$0.20$		&	$0.10$		& $0.40$			\\
$x_1 = 2$		    &	$0.10$		&	$0.20$		&	$0.00$		&	$0.00$		& $0.30$			\\
$x_1 = 3$		    &	$0.00$		&	$0.10$		&	$0.10$		&	$0.10$		& $0.30$			\\\hline
$p_{\xi_2}(x_2)$	&   $0.20$	 	&   $0.30$ 		&   $0.30$ 		&   $0.20$      & 					\\\hline
\end{tabular}
\end{center}
\vspace{2mm}

Da hier gilt, dass
\begin{equation}
p_\xi(1,1) = 0.10 \neq 0.08 = 0.40\cdot 0.20 =  p_{\xi_1}(1)p_{\xi_2}(1)
\end{equation}
sind die Zufallsvariablen $\xi_1$ und $\xi_2$ nicht unabhängig.

Die gemeinsame Verteilung von $\xi_1$ und $\xi_2$ unter der Annahme der 
Unabhängigkeit von $\xi_1$ und $\xi_2$ bei gleichen Marginalverteilungen ergibt sich zu \textcolor{darkcyan}{(Mit anderen Worten, wären $\xi_1$ und $\xi_2$ unabhäng, und die Marginalverteilungen gegeben wie oben, dann sähe die gemeinsame Verteilung so aus:)}
\vspace{2mm}

\begin{center}
\begin{tabular}{|c|cccc|c|}
\hline
$p_\xi(x_1,x_2)$	& 	$x_2 = 1$ 	& 	$x_2 = 2$ 	& 	$x_2 = 3$ 	& 	$x_2 = 4$ 	& $p_{\xi_1}(x_1)$	\\\hline
$x_1 = 1$		&	$0.08$		&	$0.12$		&	$0.12$		&	$0.08$		& $0.40$			\\
$x_1 = 2$		&	$0.06$		&	$0.09$		&	$0.09$		&	$0.06$		& $0.30$			\\
$x_1 = 3$		&	$0.06$		&	$0.09$		&	$0.09$		&	$0.06$		& $0.30$			\\\hline
$p_{\xi_2}(x_2)$	&   $0.20$	 	&   $0.30$ 		&   $0.30$ 		&   $0.20$       & 					\\\hline
\end{tabular}
\end{center}
\vspace{2mm}

\textcolor{darkcyan}{Für jede gemeinsame Wahrscheinlichkeit (jede Zelle) gilt $p_\xi(x_1,x_2) = p_{\xi_1}(x_1)p_{\xi_2}(x_2).$ z.B. $p_\xi(1,1) = p_{\xi_1}(x_1)p_{\xi_2}(x_2) = 0.40 \cdot 0.20 = 0.08$}


# \textcolor{darkcyan}{Beispiel: Unabhängigkeit diskreter Zufallsvariablen}
\footnotesize

Weiterhin ergeben sich im Falle der Unabhängigkeit von $\xi_1$ und $\xi_2$ zum 
Beispiel die bedingten Wahrscheinlichkeitsmassefunktion $p_{\xi_2|\xi_1}$ zu
\vspace{2mm}

\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|c|cccc|}
\hline
$p_{\xi_1|\xi_2}(x_1,x_2)$
& 	$x_2 = 1$
& 	$x_2 = 2$
& 	$x_2 = 3$
& 	$x_2 = 4$
\\\hline
$p_{\xi_2|\xi_1 = 1}(x_2|1)$

& 	$\frac{0.08}{0.40} = 0.2$

&	$\frac{0.12}{0.40} = 0.3$

&	$\frac{0.12}{0.40} = 0.3$

&	$\frac{0.08}{0.40} = 0.2$
\\
$p_{\xi_2|\xi_1 = 2}(x_2|2)$

&	$\frac{0.06}{0.30} = 0.2$

&	$\frac{0.09}{0.30} = 0.3$

&	$\frac{0.09}{0.30} = 0.3$

&	$\frac{0.06}{0.30} = 0.2$
\\
$p_{\xi_2|\xi_1 = 3}(x_2|3)$
%
&	$\frac{0.06}{0.30} = 0.2$
%
&	$\frac{0.09}{0.30} = 0.3$
%
&	$\frac{0.09}{0.30} = 0.3$
%
&	$\frac{0.06}{0.30} = 0.2$
\\\hline
\end{tabular}
\end{center}
\vspace{2mm}

Im Falle der Unabhängigkeit von $\xi_1$ und $\xi_2$ ändert sich die Verteilung von 
$\xi_2$ gegeben (oder im Wissen um) den Wert von $\xi_1$ also nicht und entspricht 
jeweils der Marginalverteilung von $\xi_2$. Dies entspricht natürlich der Intuition 
der Unabhängigkeit von Ereignissen im Kontext elementarer Wahrscheinlichkeiten.


# \textcolor{darkcyan}{Wdhl: Bedingte W'keit unter Unabhängigkeit}
\footnotesize

\begin{theorem}[Bedingte Wahrscheinlichkeit unter Unabhängigkeit]
\normalfont
\justifying
$(\Omega,\mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum und 
$A,B\in \mathcal{A}$ seien unabhängige Ereignisse mit $\mathbb{P}(B) > 0$. 
Dann gilt
\begin{equation}
\mathbb{P}(A|B)
= \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
= \frac{\mathbb{P}(A)\mathbb{P}(B)}{\mathbb{P}(B)}
= \mathbb{P}(A).
\end{equation}
\end{theorem}



# \textcolor{darkblue}{SKF 10}. *Unabhängigkeit von $n$ ZVen*

\large
\color{darkblue} 10. Definieren Sie den Begriff der Unabhängigkeit von $n$ Zufallsvariablen. Zufallsvariablen.

\vspace{3mm}
\color{black}
\footnotesize

\begin{definition}[$n$ unabhängige Zufallsvariablen]
\justifying
$\xi:= (\xi_1,...,\xi_n)$ sei ein $n$-dimensionaler Zufallsvektor mit Ergebnisraum 
$\mathcal{X}  = \times_{i=1}^n \mathcal{X}_i$. Die $n$ Zufallsvariablen $\xi_1,...,\xi_n$ 
heißen \textit{unabhängig}, wenn für alle $S_i \in \mathcal{X}_i, i = 1,...,n$ gilt, dass
\begin{equation}
\mathbb{P}_\xi(\xi_1 \in S_1, ...,\xi_n \in S_n) = \prod_{i=1}^n \mathbb{P}_{\xi_i}(\xi_i \in S_i).
\end{equation}
Wenn der Zufallsvektor eine $n$-dimensionale WMF oder WDF $p_\xi$ mit marginalen
WMFen oder WDFen $p_{\xi_i}, i = 1,...,n$ besitzt, dann ist die Unabhängigkeit von 
$\xi_1,...,\xi_n$ gleichbedeutend mit der Faktorisierung der gemeinsamen WMF oder WDF, also mit
\begin{equation}
p_\xi(\xi_1,...,\xi_n) = \prod_{i=1}^n p_{\xi_i}(x_i).
\end{equation}
\end{definition}



# \textcolor{darkblue}{SKF 11}. *uiv ZV*

\large
\color{darkblue} 11. Definieren Sie den Begriff $n$ unabhängig und identisch verteilter Zufallsvariablen.

\vspace{3mm}
\color{black}
\footnotesize

\begin{definition}[Unabhängig und identisch verteilte Zufallsvariablen]
\justifying
$n$ Zufallsvariablen $\xi_1,...,\xi_n$ heißen \textit{unabhängig und identisch verteilt (u.i.v.)}, wenn
\begin{itemize}
\item[(1)] $\xi_1,...,\xi_n$ unabhängige Zufallsvariablen sind, und
\item[(2)] die Marginalverteilungen der $\xi_i$ übereinstimmen, also gilt, dass
\begin{equation}
\mathbb{P}_{\xi_i} = \mathbb{P}_{\xi_j} \mbox{ für alle } 1 \le i,j \le n.
\end{equation}
\end{itemize}
Wenn die Zufallsvariablen $\xi_1,...,\xi_n$ unabhängig und identisch verteilt sind
und die $i$te Marginalverteilung $\mathbb{P}_\xi := \mathbb{P}_{\xi_i}$ ist, so 
schreibt man auch
\begin{equation}
\xi_1,...,\xi_n \sim \mathbb{P}_\xi.
\end{equation}
\end{definition}


